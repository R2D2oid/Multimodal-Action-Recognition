{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bimodal AE with Attention using C3D features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Training/Test class labels into **train_ids** and **test_ids** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from layers.AEwithAttention import AEwithAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "# import json\n",
    "\n",
    "# # num test classes\n",
    "# n_test_classes = 20\n",
    "\n",
    "# # load all video ids and labels\n",
    "# with open('data/activity_net.v1-3.min.json', 'r') as f:\n",
    "#     anet_json = json.load(f)\n",
    "    \n",
    "# # extract class labels\n",
    "# labels = set()\n",
    "# for k,v in anet_json['database'].items():\n",
    "#     if(len(v['annotations']) > 0):\n",
    "#         labels.add(v['annotations'][0]['label'])\n",
    "# all_classes = list(labels)\n",
    "\n",
    "# # pick n classes at random to form the test set\n",
    "# test_classes = [all_classes[randint(0,len(all_classes))] for i in range(n_test_classes)]\n",
    "\n",
    "# # extract set of test/training classe labels\n",
    "# test_ids = set()\n",
    "# train_ids = set()\n",
    "# for k,v in anet_json['database'].items():\n",
    "#     if(len(v['annotations']) > 0):\n",
    "#         if v['annotations'][0]['label'] in test_classes:\n",
    "#             test_ids.add(k)\n",
    "#         else:\n",
    "#             train_ids.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of videos\n",
    "# pick 100 samples at random\n",
    "\n",
    "from random import randint\n",
    "import json\n",
    "\n",
    "# num test classes\n",
    "n_test_classes = 20\n",
    "\n",
    "# load all video ids and labels\n",
    "with open('data/activity_net.v1-3.min.json', 'r') as f:\n",
    "    anet_json = json.load(f)\n",
    "    \n",
    "# extract class labels\n",
    "labels = set()\n",
    "for k,v in anet_json['database'].items():\n",
    "    if(len(v['annotations']) > 0):\n",
    "        labels.add(v['annotations'][0]['label'])\n",
    "all_classes = list(labels)\n",
    "\n",
    "# limit the number of videos being processed and number of classes to ensure we get more videos per class\n",
    "num_vids = 100\n",
    "num_classes = 10\n",
    "n_test_classes = 2\n",
    "\n",
    "# limit the number of classes \n",
    "all_classes = all_classes[:num_classes]\n",
    "\n",
    "# pick n classes at random to form the test set\n",
    "test_classes = [all_classes[randint(0,len(all_classes)-1)] for i in range(n_test_classes)]\n",
    "\n",
    "count = 0\n",
    "# extract set of test/training classe labels\n",
    "test_ids = set()\n",
    "train_ids = set()\n",
    "for k,v in anet_json['database'].items():\n",
    "    if(len(v['annotations']) > 0) and count < num_vids:\n",
    "        if v['annotations'][0]['label'] in test_classes:\n",
    "            test_ids.add(k)\n",
    "        else:\n",
    "            train_ids.add(k)\n",
    "            \n",
    "        count = count + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vids = train_ids.union(test_ids)\n",
    "selected_vids = ['v_{}'.format(v) for v in selected_vids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load C3D video features into **vids** dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "\n",
    "# fname = '../data/vid_c3d_feats/sub_activitynet_v1-3.c3d.hdf5'\n",
    "# f = h5py.File(fname,'r+')    \n",
    "\n",
    "# # extract c3d features as numpy arrays\n",
    "# vids_list = list(f.keys())\n",
    "\n",
    "# anet_c3d = {}\n",
    "# for vid in vids_list:\n",
    "#     vid_c3d_feat = np.array(f[vid]['c3d_features'])\n",
    "#     anet_c3d[vid] = vid_c3d_feat\n",
    "\n",
    "# vids = anet_c3d\n",
    "\n",
    "# # fname = 'anet_c3d.pkl'\n",
    "# # vids = pickle.load(open(fname, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "\n",
    "# fname = 'data/vid_c3d_feats/sub_activitynet_v1-3.c3d.hdf5'\n",
    "# f = h5py.File(fname,'r+')    \n",
    "\n",
    "\n",
    "# anet_c3d = {}\n",
    "# for vid in selected_vids:\n",
    "#     vid_c3d_feat = np.array(f[vid]['c3d_features'])\n",
    "#     anet_c3d[vid] = vid_c3d_feat\n",
    "\n",
    "# vids = anet_c3d\n",
    "\n",
    "fname = 'data/anet_c3d.pkl'\n",
    "vids = pickle.load(open(fname, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Captions gloVe features into **caps** dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fname = 'data/anet_captions.all.glove.pkl'\n",
    "# caps = pickle.load(open(fname, 'rb'))\n",
    "\n",
    "# # transpose feature vectors to get Lx300 dimensions\n",
    "# caps_t = {}\n",
    "# for k,v in caps.items():\n",
    "#     caps_t[k] = v.t()\n",
    "# caps = caps_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'data/anet_captions.all.glove.pkl'\n",
    "caps = pickle.load(open(fname, 'rb'))\n",
    "caps_limited = {}\n",
    "\n",
    "print(len(caps))\n",
    "for v in selected_vids:\n",
    "    if v in caps.keys():\n",
    "        temp = caps[v]\n",
    "        caps_limited[v] = temp\n",
    "    \n",
    "caps = caps_limited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any differences between caps and vids remove those vids\n",
    "\n",
    "caps_k = set(caps.keys())\n",
    "vids_k = set(vids.keys())\n",
    "\n",
    "diff = vids_k - vids_k.intersection(caps_k)\n",
    "for k in diff:\n",
    "    del vids[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num videos : {}'.format(len(vids)))\n",
    "print('num captions : {}'.format(len(caps)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose feature vectors to get Lx300 dimensions\n",
    "caps_t = {}\n",
    "for k,v in caps.items():\n",
    "    caps_t[k] = v.t()\n",
    "caps = caps_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess **vids** and **caps** to make all feature vectors the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 75 percentile to fix feature representation dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "T = []\n",
    "src = list(vids.items())\n",
    "for i in range(len(src)):\n",
    "    T.append(src[i][1].shape[0])\n",
    "\n",
    "# print 75 percentile \n",
    "T_fixed = pd.DataFrame(T).quantile(.75)\n",
    "\n",
    "T_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "src = list(caps.items())\n",
    "for i in range(len(src)):\n",
    "    L.append(src[i][1].shape[0])\n",
    "    \n",
    "# print 75 percentile \n",
    "L_fixed = pd.DataFrame(L).quantile(.75)\n",
    "\n",
    "L_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "test = {}\n",
    "for k,v in vids.items():\n",
    "    test[k] = v\n",
    "    if count>3:\n",
    "        break\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def preprocess_embeddings_dict(embeddings_dict, num_feats, T):\n",
    "    target_len = T * num_feats\n",
    "    processed_embeddings = {}\n",
    "    count = 0\n",
    "    for k, emb in embeddings_dict.items():\n",
    "        emb = emb.reshape(-1)\n",
    "        processed_emb = unify_embedding_length(emb, target_len)\n",
    "        processed_emb = processed_emb.reshape(-1, num_feats)\n",
    "        processed_embeddings[k] = processed_emb\n",
    "        count = count + 1\n",
    "        \n",
    "    return processed_embeddings\n",
    "\n",
    "# unify feat size to ensure all embeddings are 1024xT\n",
    "# if embedding is smaller augment it with zeros at the end\n",
    "# if embedding is larger crop the extra rows\n",
    "def unify_embedding_length(emb, target_len):\n",
    "    emb_len = len(emb)\n",
    "    if emb_len < target_len:\n",
    "        len_diff = target_len - emb_len\n",
    "        zero_padding = np.zeros([len_diff])\n",
    "        return torch.tensor(np.hstack((emb, zero_padding)))\n",
    "    elif emb_len > target_len:\n",
    "        return torch.tensor(emb[0:target_len])\n",
    "    else:\n",
    "        return torch.tensor(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# n_feats_v = 500\n",
    "# n_feats_t = 300\n",
    "# T_fixed = 600\n",
    "# L_fixed = 60\n",
    "\n",
    "n_feats_v = 500\n",
    "n_feats_t = 500\n",
    "T_fixed = 600\n",
    "L_fixed = 600\n",
    "\n",
    "vids_processed =  preprocess_embeddings_dict(vids, n_feats_v, T_fixed)\n",
    "caps_processed =  preprocess_embeddings_dict(caps, n_feats_t, L_fixed)\n",
    "\n",
    "vids = vids_processed\n",
    "caps = caps_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "learning_rate = 0.01\n",
    "n_epochs = 2\n",
    "\n",
    "n_filt = 4\n",
    "\n",
    "# n_feat_v = 500\n",
    "# n_feat_t = 300\n",
    "# T = 600\n",
    "# L = 60\n",
    "\n",
    "n_feat_v = 500\n",
    "n_feat_t = 500\n",
    "T = 600\n",
    "L = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with paired v,t data using recons, joint, cross, cycle loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input ###\n",
    "# vids\n",
    "# caps\n",
    "    \n",
    "model_v = AEwithAttention(n_feat_v, T, n_filt)\n",
    "model_t = AEwithAttention(n_feat_t, L, n_filt)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_v = torch.optim.SGD(model_v.parameters(), lr = learning_rate, momentum = True)\n",
    "optimizer_t = torch.optim.SGD(model_t.parameters(), lr = learning_rate, momentum = True)\n",
    "\n",
    "optimizer_E_v = torch.optim.SGD(model_v.encoder_.parameters(), lr = learning_rate, momentum = True)\n",
    "optimizer_E_t = torch.optim.SGD(model_t.encoder_.parameters(), lr = learning_rate, momentum = True)\n",
    "\n",
    "optimizer_G_v = torch.optim.SGD(model_v.decoder_.parameters(), lr = learning_rate, momentum = True)\n",
    "optimizer_G_t = torch.optim.SGD(model_t.decoder_.parameters(), lr = learning_rate, momentum = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(n_epochs):\n",
    "    counter = 1\n",
    "    for i in vids.keys():\n",
    "        # Forward pass\n",
    "        v = vids[i]\n",
    "        t = caps[i]\n",
    "        \n",
    "        v = torch.tensor(v).float()\n",
    "        t = torch.tensor(t).float()\n",
    "        \n",
    "        dim = v.shape\n",
    "\n",
    "        # Compute recons loss \n",
    "        loss_recons_v = criterion(model_v(v).reshape(dim[0], dim[1]), v)\n",
    "        loss_recons_t = criterion(model_t(t).reshape(dim[0], dim[1]), t)\n",
    "        loss_recons = loss_recons_v + loss_recons_t\n",
    "        # the following losses require paired video/caption data (v and t)\n",
    "        # model_v and model_t are the corresponding models for video and captions respectively\n",
    "\n",
    "        # Compute joint loss\n",
    "        loss_joint = criterion(model_v.encoder(v), model_t.encoder(t))\n",
    "\n",
    "        # Compute cross loss\n",
    "        loss_cross1 = criterion(model_t.decoder(model_v.encoder(v)).reshape(dim[0], dim[1]), t)\n",
    "        loss_cross2 = criterion(model_v.decoder(model_t.encoder(t)).reshape(dim[0], dim[1]), v)\n",
    "        loss_cross = loss_cross1 + loss_cross2\n",
    "\n",
    "        # Compute cycle loss\n",
    "        loss_cycle1 = criterion(model_t.decoder(model_v.encoder(model_v.decoder(model_t.encoder(t)))).reshape(dim[0], dim[1]), t)\n",
    "        loss_cycle2 = criterion(model_v.decoder(model_t.encoder(model_t.decoder(model_v.encoder(v)))).reshape(dim[0], dim[1]), v)\n",
    "        loss_cycle = loss_cycle1 + loss_cycle2\n",
    "\n",
    "        # set hyperparams \n",
    "        a1, a2, a3 = 1, 1, 1\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = loss_recons + a1 * loss_joint + a2 * loss_cross + a3 * loss_cycle\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        optimizer_v.zero_grad()\n",
    "        optimizer_t.zero_grad()\n",
    "        optimizer_E_v.zero_grad()\n",
    "        optimizer_E_t.zero_grad()\n",
    "        optimizer_G_v.zero_grad()\n",
    "        optimizer_G_t.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer_v.step()\n",
    "        optimizer_t.step()\n",
    "        optimizer_E_v.step()\n",
    "        optimizer_E_t.step()\n",
    "        optimizer_G_v.step()\n",
    "        optimizer_G_t.step()\n",
    "        \n",
    "        print ('Epoch[{}/{}], Step[{}/{}] Loss: {}\\n'.format(epoch + 1, n_epochs, counter, len(vids), loss.item()))\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "# torch.save(model.state_dict(), 'out/model.sd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
